<!doctype html>
<meta charset="utf-8">
<script src="public/template.v1.js"></script>

<script type="text/front-matter">
  title: "Understanding the ANS Compressor"
  description: "understanding the Asymmetric Numeral Systems Compressor"
  authors:
  - Kedar Tatwawadi: https://github.com/kedartatwawadi
  affiliations:
  - Stanford University: http://stanford.edu
</script>


<!-- Katex -->
<script src="public/assets/lib/auto-render.min.js"></script>
<script src="public/assets/lib/katex.min.js"></script>
<link rel="stylesheet" href="public/assets/lib/katex.min.css">
<link rel="stylesheet" type="text/css" href="assets/widgets.css">

<!-- Required -->
<script src="public/assets/lib/lib.js"></script>
<script src="public/assets/utils.js"></script>
<script src="public/rANS.js"></script>
<script>
  var renderQueue = [];
  function renderMath(elem) {
    // renderMathInElement(
    //     elem,
    //     {
    //         delimiters: [
    //             {left: "$$", right: "$$", display: true},
    //             {left: "$", right: "$", display: false},
    //         ]
    //     }
    // );
  }
  var deleteQueue = [];
  function renderLoading(figure) {
    var loadingScreen = figure.append("svg")
    .style("width", figure.style("width"))
    .style("height", figure.style("height"))
    .style("position","absolute")
    .style("top", "0px")
    .style("left","0px")
    .style("background","white")
    .style("border", "0px dashed #DDD")
    .style("opacity", 1)
    return function(callback) { loadingScreen.remove() };
  }
</script>
<div id="math-cache" style="display: none;">
  <dt-math class="star">\star</dt-math>
  <dt-math class="plus">+</dt-math>
  <dt-math class="minus">-</dt-math>
  <dt-math class="equals">=</dt-math>
  <dt-math class="alpha">\alpha</dt-math>
  <dt-math class="lambda">\lambda</dt-math>
  <dt-math class="beta">\beta</dt-math>
  <dt-math class="r">R</dt-math>
  <dt-math class="alpha-equals">\alpha=</dt-math>
  <dt-math class="beta-equals">\beta=</dt-math>
  <dt-math class="beta-equals-zero">\beta = 0</dt-math>
  <dt-math class="beta-equals-one">\beta=1</dt-math>
  <dt-math class="alpha-equals-one-over-lambda-i">\alpha = 1/\lambda_i</dt-math>
  <dt-math class="model">\text{model}</dt-math>
  <dt-math class="p">0 p_1</dt-math>
  <dt-math class="phat">0 \bar{p}_1</dt-math>
  <dt-math class="two-sqrt-beta">2\sqrt{\beta}</dt-math>
  <dt-math class="lambda-i">\lambda_i</dt-math>
  <dt-math class="lambda-i-equals-zero">\lambda_i = 0</dt-math>
  <dt-math class="alpha-gt-one-over-lambda-i">\alpha > 1/\lambda_i</dt-math>
  <dt-math class="max-sigma-one">\max\{|\sigma_1|,|\sigma_2|\} > 1</dt-math>
  <dt-math class="x-i-k">x_i^k - x_i^*</dt-math>
  <dt-math class="xi-i">\xi_i</dt-math>
  <dt-math class="beta-equals-one-minus">\beta = (1 - \sqrt{\alpha \lambda_i})^2</dt-math>
</div>
<script>
  function MathCache(id) {
    return document.querySelector("#math-cache ." + id).innerHTML;
  }
</script>
<svg style="display: none;">
  <g id="pointerThingy">
    <circle fill="none" stroke="#FF6C00" stroke-linecap="round" cx="0" cy="0" r="14"/>
    <circle fill="#FF6C00" cx="0" cy="0" r="11"/>
    <path id="XMLID_173_" fill="#FFFFFF" d="M-3.2-1.3c0-0.1,0-0.2,0-0.3c0-0.1,0-0.2,0-0.3c-0.6,0-1.2,0-1.8,0c0,0.6,0,1.2,0,1.8
      c0.2,0,0.4,0,0.6,0c0-0.4,0-0.8,0-1.2c0,0,0.1,0,0.1,0c0.3,0,0.5,0,0.8,0C-3.4-1.3-3.3-1.3-3.2-1.3c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0-0.1c0-1.6,0-3.2,0-4.8c0-0.6,0-1.2,0-1.8c0,0,0,0,0.1,0
      c0.3,0,0.7,0,1,0c0.1,0,0.1,0,0.2,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0C-2-7.2-2-7-2-6.8c0,0,0,0-0.1,0c-0.2,0-0.3,0-0.5,0
      c0,0,0,0-0.1,0c0,1.8,0,3.6,0,5.5c-0.2,0-0.3,0-0.4,0C-3.1-1.3-3.2-1.3-3.2-1.3z M1.1-3.7C1-3.8,1-3.8,1.1-3.7C1-4,1-4.1,1-4.3
      c0,0,0,0,0-0.1c-0.4,0-0.8,0-1.2,0c0-0.8,0-1.6,0-2.4c-0.2,0-0.4,0-0.6,0c0,1.8,0,3.6,0,5.5c0.2,0,0.4,0,0.6,0c0-0.8,0-1.6,0-2.4
      c0,0,0.1,0,0.1,0C0.3-3.7,0.6-3.7,1.1-3.7C1-3.7,1-3.7,1.1-3.7C1.1-3.7,1-3.7,1.1-3.7c0,0.8,0,1.6,0,2.3c0,0,0,0.1,0,0.1
      c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.4,0,0.8,0,1.2,0c0,0.8,0,1.6,0,2.4c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.2,0,0.4,0,0.6,0
      c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0,0,0.1,0,0.1c0.2,0,0.4,0,0.5,0c0,0,0.1,0,0.1,0.1c0,0.2,0,0.5,0,0.7c0,1.1,0,2.3,0,3.4
      c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0,0,0,0,0c0,0.6,0,1.1,0,1.7c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c-1.6,0-3.2,0-4.9,0c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0C-2,3.8-2,3.4-2,3c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c0.2,0,0.4,0,0.6,0C-2,4.8-2,5.4-2,6c2,0,4.1,0,6.1,0c0-0.1,0-0.2,0-0.3c0-0.5,0-0.9,0-1.4c0-0.1,0-0.1,0-0.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.4,0-0.9,0-1.3c0-0.1,0-0.3,0-0.4c0.1,0,0.2,0,0.3,0c0.1,0,0.2,0,0.3,0c0-1.4,0-2.8,0-4.3
      c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0c0-0.2,0-0.4,0-0.6
      c-0.1,0-0.2,0-0.3,0c-0.4,0-0.9,0-1.3,0C1.2-3.7,1.1-3.7,1.1-3.7z M-3.2,1.8c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.3,0-0.6,0-1c0-0.1,0-0.1,0-0.2C-2.8,1.8-3,1.8-3.2,1.8c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0.2,0,0.5,0,0.7
      c0,0,0,0.1,0.1,0.1c0.1,0,0.2,0,0.3,0C-3.4,1.8-3.3,1.8-3.2,1.8z"/>
    <path id="XMLID_172_" fill="#FFFFFF" d="M4.1,4.2C4.1,4.2,4.1,4.2,4.1,4.2c0-0.6,0-1.2,0-1.8c0,0,0,0,0,0c0.2,0,0.4,0,0.6,0
      c0,0,0-0.1,0-0.1c0-1.1,0-2.3,0-3.4c0-0.2,0-0.5,0-0.7c0,0,0-0.1-0.1-0.1c-0.2,0-0.4,0-0.5,0c0,0,0-0.1,0-0.1c0-0.1,0-0.3,0-0.4
      c0,0,0-0.1,0-0.1c-0.2,0-0.4,0-0.6,0c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0-0.8,0-1.6,0-2.4c-0.4,0-0.8,0-1.2,0
      c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0,0,0-0.1,0-0.1c0-0.7,0-1.5,0-2.2c0,0,0-0.1,0-0.1l0,0c0.1,0,0.2,0,0.2,0
      c0.4,0,0.9,0,1.3,0c0.1,0,0.2,0,0.3,0c0,0.2,0,0.4,0,0.6c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,1.4,0,2.8,0,4.3c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0.1,0,0.3,0,0.4c0,0.4,0,0.9,0,1.3
      c0,0.1,0,0.1-0.1,0.1C4.5,4.2,4.3,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_171_" fill="#FFFFFF" d="M4.1,4.2c0,0.1,0,0.1,0,0.2c0,0.5,0,0.9,0,1.4c0,0.1,0,0.2,0,0.3C2.1,6,0,6-2,6
      c0-0.6,0-1.2,0-1.8c-0.2,0-0.4,0-0.6,0c0-0.4,0-0.8,0-1.2C-2.4,3-2.2,3-2,3c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.6,0c0,0.4,0,0.8,0,1.2
      c1.6,0,3.2,0,4.9,0c0-0.4,0-0.8,0-1.2C3.7,4.2,3.9,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_170_" fill="#FFFFFF" d="M-2-6.8c0,0.6,0,1.2,0,1.8c0,1.6,0,3.2,0,4.8c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6l0,0c0.1,0,0.1,0,0.2,0c0.1,0,0.3,0,0.4,0c0-1.8,0-3.6,0-5.5
      c0,0,0.1,0,0.1,0C-2.4-6.8-2.2-6.8-2-6.8C-2.1-6.8-2-6.8-2-6.8L-2-6.8z"/>
    <path id="XMLID_169_" fill="#FFFFFF" d="M1.1-3.7C1-3.7,1-3.7,1.1-3.7c-0.4,0-0.8,0-1.2,0c0,0,0,0-0.1,0c0,0.8,0,1.6,0,2.4
      c-0.2,0-0.4,0-0.6,0c0-1.8,0-3.6,0-5.5c0.2,0,0.4,0,0.6,0c0,0.8,0,1.6,0,2.4c0.4,0,0.8,0,1.2,0c0,0,0,0.1,0,0.1C1-4.1,1-4,1.1-3.7
      C1-3.8,1-3.8,1.1-3.7L1.1-3.7z"/>
    <path id="XMLID_168_" fill="#FFFFFF" d="M-3.2,1.8c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0-0.1,0-0.1-0.1c0-0.2,0-0.5,0-0.7
      c0-0.1,0-0.3,0-0.4c0,0,0,0,0-0.1c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0
      C-3.2,0.9-3.2,1.3-3.2,1.8c0.2,0,0.4,0,0.6,0c0,0.1,0,0.1,0,0.2c0,0.3,0,0.6,0,1C-2.6,3-2.7,3-2.7,3c-0.2,0-0.3,0-0.5,0
      C-3.2,2.6-3.2,2.2-3.2,1.8z"/>
    <path id="XMLID_167_" fill="#FFFFFF" d="M-3.2-1.3c-0.1,0-0.2,0-0.3,0c-0.3,0-0.5,0-0.8,0c0,0,0,0-0.1,0c0,0.4,0,0.8,0,1.2
      c-0.2,0-0.4,0-0.6,0c0-0.6,0-1.2,0-1.8c0.6,0,1.2,0,1.8,0c0,0.1,0,0.2,0,0.3C-3.2-1.5-3.2-1.4-3.2-1.3L-3.2-1.3z"/>
    <path id="XMLID_166_" fill="#FFFFFF" d="M-2-6.8C-2-7-2-7.2-2-7.4c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c-0.1,0-0.1,0-0.2,0
      C-1.3-6.8-1.6-6.8-2-6.8C-2-6.8-2-6.8-2-6.8L-2-6.8z"/>
  </g>
</svg>

<dt-article class="centered">
  <h1>What is ANS?</h1>
  <h2>Understanding the new entropy coder: Asymmetric Numeral Systems</h2>
  <dt-byline></dt-byline>
  <p>Recently while attending a talk at the Stanford Compression Forum, I heard about a new class of entropy coders called the Asymmetric Numeral Systems coders (ANS). Considering Arithmetic coding and Huffman coding <dt-cite key="cover2012elements"></dt-cite> have essentially resolved the problem of entropy coding, it is surprising that there is a new development in the field of entropy coding. Today, just a few years after the original paper <dt-cite key="duda2013asymmetric"></dt-cite> came out, ANS has already been incorporated into numerous compressors, including Zstandard (facebook), LZFSE (Apple), CRAM (Genomic SAM file compression) etc. I was curious to understand what exactly did this new kid on the block did, but found the original paper pretty difficult to follow. Thanks to some amazing and detailed blog posts, I was able to make some sense out of the paper! The ANS Coder is not a single compressor but a class of entropy coders for a given distribution. The most important thing they achieve is obtaining very accurate compression at very high compression and decompression speeds (like Pied Piper, ANS significantly improves upon the Weissman score :) ). </p>

  <p> In this series of posts, we will try to improve our understanding of the ANS family of coders. But before we do that, lets revise the two of the most important entropy coders we have: Huffman and Arithmetic Coders. </p>

  <h2> Huffman and Arithmetic Coders </h2>
  <p>
  Huffman coding (and for that matter any prefix-free codes) are the fastest entropy coders, as all they do is perform table-lookup for the unique prefix-free code for the input symbol. The prefix-free nature of the codes also allows for efficient decoding using a binary tree. However, in some cases the Huffman coder can be significantly suboptimal in terms of compression, with a gap with the entropy as high as <dt-math> 1 bit/symbol </dt-math>. One can also imagine huffman coding as a Finite State Entropy coder (FSE) with a single state: For every input alphabet, the encoder outputs the corresponding prefix-free code (from the lookup-table) and transitions back to the same state. </p>

  <p> On the other extreme we have the family of Arithmetic coders. Arithmetic coding can be thought of as an encoder which implicitly represents the entire input as a single state from a huge Finite-State automata (whose size is asymtotically exponential in the length of the input). Arithmetic coding is optimal in terms of its compression, but has slower compression/decompression speeds. </p>

<!--   If you brush up your memory (or look here: <dt-cite key="gregor2015draw"></dt-cite>) , you would remember that Arithmetic coding subdivides the <i>range</i> which it maintains as a state and thus uses the <i>division</i> operation at every step (both during compression and decompression), which is the main reason for it being slow. 
 -->
  <p> The ANS family of coders forms the bridge between these two extremes of entropy coders, where we can design a k-state FSE coder (for a wide range of <dt-math> k </dt-math> values), which results in better compression optimality than Huffman coding with a slight decrease in the speed. This ability of graceful tradeoff between the compression vs speed is crucial to what makes the ANS family so useful in practice.  We will first look at one member of the ANS family of coders: <b>rANS</b>, which is an acronym for range-ANS due to its similarities to the arithmetic encoder.   
  </p>

 <h2> rANS </h2>
 <p>
 Lets consider a simple example: Say we have sequence of digits (in the range <dt-math> [0,\ldots,9] </dt-math>), <dt-math> S = \{ 3,2,0,8,9,1 \} </dt-math> which we want to represent by a single integer state, how can we do that? We can simply form a single number <dt-math> X_6 = 320891 </dt-math>. What we are doing is essentially the following: 
 <dt-math block>
  \begin{aligned} 
  X_0 &= 0 \\
  X_1 &= X_0*10 + 3 \\
  X_2 &= X_1*10 + 2 \\
    &\ldots \\
  X_6 &= X_5*10 + 1 
  \end{aligned}
 </dt-math>

 The final state <dt-math> X_6 </dt-math> contains all the information necessary to recover the original string. We can encode and store this state using bits by simply converting the state into binary using <dt-math> \log_2 ( X_6 ) </dt-math> bits: thus, larger the state, the more expensive is the bit-representation. These bits can then be read to decode <dt-math> X_6 </dt-math>, from which we can recursively decode <dt-math> S </dt-math> in the reverse order as follows:
  
 <dt-math block> 
 \begin{aligned}
 X_6 &= 320891 \\
 X_5 &= \lfloor  X_6/10 \rfloor, s_6 =  mod(X_6,10) \\
 X_4 &= \lfloor  X_5/10 \rfloor, s_5 = mod(X_5,10) \\
 &\ldots  \\
 X_0 &= 0, s_1 = 3 \\
 \end{aligned}
 </dt-math>

<!--  
 s_5 &= X_5 % 10,  X_4 &= floor \left( \frac{X_5}{10} \right)\\
 &\ldots  \\
 s_1 &= 3, X_0 &= 0 \\ -->
 This looks simple. But, the question to ask is, if this representation of <dt-math> S </dt-math> is optimal? Are we using the minimum possible number of bits? Well, if we look at the encoding, at time step <dt-math> t </dt-math>, we are scaling the state by approximately <dt-math> 10 </dt-math> times (since: <dt-math> X_t \approx X_{t-1}*10 </dt-math>). This is reasonable if all the symbols <dt-math> [0,\ldots,9] </dt-math> are equiprobable, as we are essentially spending <dt-math> \log_2 10 </dt-math> bit per encoded symbol. In the case where we have non-uniform symbol distribution, this would be suboptimal as its entropy can be far less than <dt-math> \log_2 10 </dt-math>. Can do modify the structure to improve the performance for non-uniform distributions? </p>

 <p>
 One idea can be to scale more frequent symbols with a factor smaller than <dt-math> 10 </dt-math>, while the less frequent ones by a larger factor. This is infact the core idea behind the rANS construction, making our standard numeral system asymmetric to achieve a more efficient representation. the rANS encoding algorithm is a very ingenious adaptation of this simple idea! </p>
 
 
  <!-- <li> rANS keeps track of the input using a single integer state. Let <dt-math> X_t </dt-math> represent the integer state of rANS after it has looked at <dt-math> t </dt-math> input symbols. <dt-math> X_0 = 1 </dt-math> is the initial state of rANS. <dt-math> X_t </dt-math> is a function of the previous state <dt-math> X_{t-1} </dt-math> and the current symbol <dt-math> s_t </dt-math>
    <dt-math block>  X_t = C_{rANS}(X_{t-1},s_t)</dt-math>
  <li> The compression output is the state <dt-math> X_n </dt-math> represented using <dt-math> ceil(\log_2 X_n) </dt-math> bits. <dt-math> X_n </dt-math> (along with the number of symbols encoded: <dt-math> n </dt-math>) is used to decode the entire input string <dt-math> S </dt-math>
    <dt-math block>  s_t, X_{t-1} = D_{rANS}(X_{t})</dt-math> -->

   <h3> Notation </h3>
   <p>
   Before we describe rANS, some notation: 
  <ol>
  <li> Let <dt-math>S = (s_1,s_2,s_3,\ldots, s_n) </dt-math> be the input string of symbols from the alphabet set <dt-math> \mathcal{A} = \{a_1,a_2,\ldots,a_k \}</dt-math> of size <dt-math>k</dt-math>
  <li> We assume the data comes from a distribution characterized by the frequency counts <dt-math> \mathcal{F} = \{ F_{a_1}, F_{a_2},\ldots, F_{a_k} \} </dt-math>, which are integers proportional to the probability mass distribution <dt-math>\{p_1,p_2,\ldots,p_k\}</dt-math> of the symbols.
    <br>
  Let <dt-math> M = \sum_{i=1}^k F_i </dt-math>. Then, <dt-math> p_i = \frac{F_{a_i}}{M} </dt-math>
  <li> We also define the cumulative frequency counts <dt-math> C_{a_i} = \sum_{j=1}^{i-1} F_{a_j} </dt-math>, which correspond to the cumulative distribution of the symbols
  </ol>
  </p>
  <p> <b>Example:</b> Let the alphabet be <dt-math> \mathcal{A} = \{ A,B,C\} </dt-math>. Then, we can consider input generated using frequency counts <dt-math> F = \{ 3, 3, 2\} </dt-math>, which corresponds to probabilities <dt-math> \{ \frac{3}{8}, \frac{3}{8}, \frac{2}{8} \} </dt-math>. 
  <br> The cumulative frequency counts in this case corresponds to <dt-math> C = \{ 0, 3, 6 \} </dt-math>
  </p>

   <p>
   rANS keeps track of the input using a single integer state. Let <dt-math> X_t </dt-math> represent the integer state of rANS after it has looked at <dt-math> t </dt-math> input symbols. We initialize the rANS state to be, <dt-math> X_0 = 0 </dt-math>. rANS updates the state <dt-math> X_t </dt-math> based on the previous state <dt-math> X_{t-1} </dt-math> and the current symbol <dt-math> s_t </dt-math>.
    <dt-math block>  X_t = C_{rANS}(X_{t-1},s_t)</dt-math> 
   The compression output is the final state: <dt-math> X_n </dt-math> represented using <dt-math> \lceil \log_2 X_n \rceil </dt-math> bits. <dt-math> X_n </dt-math> (along with the number of symbols encoded: <dt-math> n </dt-math>) is used to decode the entire input string <dt-math> S </dt-math>. The decoder works in an exactly reverse direction: recursively extracting the previous state, and the last encoded symbol. 
   <dt-math block>  s_t, X_{t-1} = D_{rANS}(X_{t})</dt-math>
   </p> 

   <h3> rANS Encoding</h3>
   <p>
   So here it goes: 
   </p>
  
   <p>
   <dt-math block> 
      X_t = \left\lfloor \frac{X_{t-1}}{F_{s_t}} \right\rfloor * M + C_{s_t} + mod(X_{t-1}, F_{s_t}) 
   </dt-math>
    
   Thats all! This is the encoding step for the rANS. 

   <h4> rANS Encoding Example</h4>
   <p>
   Symbol Counts, <dt-math> \mathcal{F} </dt-math> <input type="text" id="symbol_counts" value="3,3,2"> <br>
   Input Symbol String: <input type="text" id="input_text" value="0,1,0,2,2,0,2,1,2"> <br>
   <button onclick="rANS_encoder()">Try it</button> 
   </p>
   <p id="demo"></p>
   <p>
   Lets also state the decoder for completeness. </p>
   
  <h3> rANS Decoding</h3>
   <p>
   Let <dt-math> C\_inv(y) </dt-math> be the inverse function of the cumulative frequency, where: <dt-math> C\_inv(y) = a_i </dt-math>, if <dt-math> C_{a_i} \leq y  < C_{a_{i+1}} </dt-math>. Then, the decoder, <dt-math>D_{rANS}(X_{t})</dt-math> is defined as follows:

   <dt-math block> 
      \begin{aligned}
      slot &= mod(X_t,M) \\
      s_t  &= C\_inv(slot) \\
      X_{t-1} &=  \left\lfloor \frac{X_t}{M} \right\rfloor * F_{s_t} + slot - C_{s_t} 
      \end{aligned}
   </dt-math>

   <h4> rANS Decoding Example </h4>
   <p>
   Symbol Counts, <dt-math> \mathcal{F} </dt-math> <input type="text" id="symbol_counts_decoder" value="3,3,2"> <br>
   State <input type="text" id="state_decoder" value="17910"> <br>
   Number of Encoded Symbols <input type="text" id="num_decoder" value="9"> <br>
   <button onclick="rANS_decoder()">Try it</button> 
   </p>
   <p id="rANS_decoder"></p>

   </p>

   


   


   <p>
   One can understand rANS encoding as a 2-step process: In the first step, we choose a <dt-math> M </dt-math> sized block. For a state <dt-math> X_{t-1} </dt-math>, we choose the block with  <dt-math> blockId =  \lfloor {X_{t-1}}/{F_{s_t}} \rfloor </dt-math>. Once the <dt-math> blockId </dt-math> is fixed, we choose a <dt-math> slot </dt-math>, out of the <dt-math> M </dt-math> allowed integers from the block. For the next symbol being <dt-math> s_t </dt-math>, the <dt-math> slot </dt-math> can be chosen in the range <dt-math> [C_{s_t},C_{s_{t} + 1}-1] </dt-math>. The next state <dt-math> X_t </dt-math> then is composed of as: <dt-math> X_t = blockId*M + slot </dt-math> </p>

   <p>
    Having distinct ranges for every symbol makes it possible to decode <dt-math> s_t </dt-math>, by only looking at: 
    <dt-math> slot = mod(X_t,M) </dt-math> during the decoding, using the <dt-math> C\_inv </dt-math> function. We can also retrieve the  <dt-math> blockId </dt-math> from the state <dt-math> X_t </dt-math> using: <dt-math> blockId = \lfloor X_t/M \rfloor </dt-math>. As <dt-math> blockId = \lfloor X_{t-1}/F_{s_t} \rfloor </dt-math>, the only missing piece of information required to retrieve <dt-math> X_{t-1} </dt-math> is <dt-math> mod(X_{t-1}, F_{s_t}) </dt-math>. Amazingly we can accomodate this information during the encoding, by choosing the <dt-math> slot </dt-math> to be <dt-math> C_{s_t} + mod(X_{t-1}, F_{s_t}) </dt-math>, which lies in the allowed range of <dt-math> [C_{s_t},C_{s_{t} + 1}-1] </dt-math>.
    </p>
   
  
  <!-- <p>
  We also present python function scripts (which is more of a pseudo-code) to play around with. 

  <dt-code block language="python">
  # rANS Encoder code. Not optimized in any way
  # Mainly for playing around 
  def rANS_encoder(s_input, symbol_counts):
    total_counts = np.sum(symbol_counts)  #Represents M
    cumul_sum = np.insert(np.cumsum(symbol_counts),0,0) #Cumul_sum represents the cumulative frequencies
    num_symbols = len(s_input) #the length of the input
    
    state = 1 #Initialize the state
    for s in s_input:
      s_count = symbol_counts[s] #current symbol count/frequency
      state = (state/s_count)*total_counts + cumul_sum[s] + (state % s_count) #Main encoding step

    return state, num_symbols
   </dt-code>

   <dt-code block language="python">
  #rANS Decoder code. Not optimized in any way
  #Mainly for playing around 
  def rANS_decoder(state, num_symbols, symbol_counts):
    total_counts = np.sum(symbol_counts)  #Represents M
    cumul_sum = np.insert(np.cumsum(symbol_counts),0,0) #Cumul_sum represents the cumulative frequencies

    #The Cumulative frequency inverse function
    def cumul_inverse(y): 
      for i,_s in enumerate(cumul_sum): 
        if y < _s: return i-1

    s_decoded = []  #Initialize a list to hold the decoded symbols
    while len(s_decoded) < num_symbols: #continue decoding until all symbols decoded
      slot = state % total_counts #compute the slot     
      s = cumul_inverse(y) #decode the symbol
      s_decoded.append(s) 
      state = (state/total_counts)*symbol_count[s] + slot - cumul_sum[s] #update the state

    return x_decoded
   </dt-code>
 -->
  <h3> Optimality of rANS </h3>
  <p>
  Now that we have convinced ourselves that rANS indeed is a lossless codec, lets see why rANS achieves the optimal compression ratio. Notice that: 
  
  <dt-math block>  
    \begin{aligned}
    \frac{X_t}{X_{t-1}} &\approx p(s_t) = \frac{M}{F_{s_t}} \\
    {X_n} &\approx \prod_{t=1}^n p(s_t) \\
    \log_2(X_n) &\approx \sum_{t=1}^{n} \log_2 \frac{1}{p(s_t)} \approx n H(P) \\
    \end{aligned}
  </dt-math>

  For every symbol <dt-math> s_t </dt-math>, we use approximately <dt-math>  \log_2 \frac{1}{p(s_t)} </dt-math> bits for its representation. This is optimal as its expected value is the entropy of the source. Thus, this gives a nice justification for the optimality of rANS, which can be made rigorous by some more math. We will leave the proof here, but for those interested can look at the footnote for the complete proof. 
  </p>

  <h3> Practical Aspects </h3>
  <p> 
  Lets take a look at implementing rANS in practice! 
  <ol>
  <li><b> Choice of <dt-math> M </dt-math> </b>: Notice that the decoding can be made very fast by choosing <dt-math> M </dt-math> to be of the form: <dt-math> 2^r </dt-math>. The modulus operation (<dt-math> slot = mod(X_t, M)) </dt-math> in the decoding then corresponds to picking the last <dt-math> r </dt-math> bits, while the division operation (<dt-math> \lfloor X_t/M \rfloor </dt-math>) becomes equivalet to bit-shifts both of which are very fast. 
  <li><b> Choice of <dt-math> \mathcal{F} </dt-math> frequencies </b>: Note that one operation we need to perform is choosing frequency counts <dt-math> F_a </dt-math> which correctly approximate the true probabilities <dt-math> q_a \approx F_a/M = p_a</dt-math>. This is one crucial step of inaccuracy in the ANS design (unlike Arithmetic encoding), as we know that designing a code for <dt-math> p_a </dt-math> distribution, when the true distribution is <dt-math> q_a </dt-math> always leads to a loss equivalent to the KL-divergence: <dt-math> D(q_a||p_a) </dt-math>. 
  <li><b> Reverse Encoding </b>: Notice that unlike Arithmetic coder, rANS decodes the input in a <i>reverse</i> order. One simple solution to this problem is processing the inputs in blocks and parsing the input in a reverse direction during encoding. This would add to the latency in the encoding, but practically seems alright as input is generally processed and transmitted in blocks anyway. 
  <li><b> Adaptive context-based rANS</b>: One of the most popular usages of Arithmetic coding is with context-based modeling. A lot of modeling techniques such as PPM, CTW, Markov modeling etc. employ arithmetic coding over a distribution modeled using the past samples. This modeling becomes tricky for rANS due to its reverse order encoding. rANS essentially needs to look <i> ahead </i> of the current reverse-order input during the encoding, so that the decoder has the necessary information to figure out the context model
 <!--  <li><b> Alias mapping instead of cumulative inverse </b>: Recall that the encoding, for the input symbol <dt-math> s_t </dt-math>, we were mapping <dt-math> slot </dt-math> to the range <dt-math> [C_{s_t, C_{s_{t+1}} - 1] </dt-math>. During the decoding, in the <dt-math> C\_inv </dt-math> function, we need to perform binary search to locate the appropriate range to decode <dt-math> s_t </dt-math>, which can be relatively slow as compared with other rANS operations. If we understand the encoding, then it is clear that we need to map the <dt-math> slot </dt-math> to a range of size <dt-math> F_{s_t} </dt-math>, however we do not need to map it to a continuous range.  -->
  <li><b> Finite Precision Arithmetic</b>: In practice, as we will be using finite precision arithmetic, the <dt-math> state </dt-math> variable would be bounded by something like: <dt-math> 2^{64} </dt-math>. Although this seems large, rANS encoding would very rapidly reach this limit (infact exponentially). Thus, we need modify the rANS construction to have the state bounded in some range. We will next look at this important variant, called  <i>streaming-rANS</i>. 
  </ol>
  </p>

  <h2> Streaming-rANS [IN PROGRESS]</h2>
  <p>
  To apply rANS on a streaming input (or even a moderately large input), we can come up with some very simple modifications: One simple idea is to set a maximum state size of say, <dt-math>H = 2^{64} </dt-math> and encode as many symbols so that the state is not larger than <dt-math>H</dt-math>. In this case, we separately need to store the number of encoded symbols as well. Another variation is to encode a fixed number of input symbols (say <dt-math>20</dt-math>), but at the same time ensure that the state does not overflow. One issue with both of these variants that, we lose something everytime we chose a new block which can be significantly suboptimal. Is there a way to utilize these partial bits? 
  </p>
  <p>
  One idea is to force the state to lie in a specific interval <dt-math> \mathcal{I} = [L,H] </dt-math> at every timestep <dt-math>t</dt-math>. Let the state <dt-math>X_{t-1}</dt-math> at timestep <dt-math>{t-1}</dt-math> be in the range <dt-math>\mathcal{I}</dt-math>. Now, if on encoding the next symbol <dt-math>s_t</dt-math>, the state <dt-math>X_t = C_{rANS}(X_{t-1},s_t)</dt-math> is going to be in the range <dt-math>\mathcal{I}</dt-math>, then well and good :), we don't have to do anything. However, if not, then we need to decrease the value of the state <dt-math>X_{t-1}</dt-math> appropriately. Let <dt-math> \mathcal{I}_a = [L_a,H_a] </dt-math> be the interval corresponding to alphabet <dt-math>a</dt-math>, so that for any state <dt-math>z \in I_a</dt-math> on encoding we have <dt-math>C_{rANS}(z,a) \in I</dt-math>. Then, we need to modify of the state <dt-math>X_{t-1}</dt-math>, so that the mapped value <dt-math>X'_{t-1}</dt-math> lies in the range <dt-math>\mathcal{I}_{s_t}</dt-math>. For example:
  <dt-math block>
  \begin{aligned}
  \mathcal{A} &= \{ A,B,C\}, \mathcal{F} = \{ 3, 3, 2\} \\
  \mathcal{I} &= \{ 8,9,10,\ldots,14,15\} \\ 
  \mathcal{I}_A &= \{ 3,4,5\} \\
  \mathcal{I}_B &= \{ 3,4,5\} \\
  \mathcal{I}_C &= \{ 2,3\} 
  \end{aligned}
  </dt-math>

  Notice that, as the size of the interval <dt-math>|\mathcal{I}_a|</dt-math> is in general smaller than <dt-math>|\mathcal{I}|</dt-math>, due to which there is no one-to-one mapping between the mapped value <dt-math>X'_{t-1}</dt-math> and the original state <dt-math>X_{t-1}</dt-math>. Thus, during the decoding, although the decoder will be able to figure out <dt-math>s_t, X'_{t-1} = D_{rANS}(X_t)</dt-math>, it will need some additional information to correctly figure out the state <dt-math>X_{t-1}</dt-math> from <dt-math>X'_{t-1}</dt-math>. Is there an elegant way to perform this mapping (and the reverse mapping at the decoder)? It seems there is! Jarek Duda in the paper: <dt-cite key="duda2013asymmetric"></dt-cite> gives a simple but a very ingenious way to achieve this! 
  </p> 
   
  <h3> State Mapping </h3>
  <p>
  One simple idea of mapping the state <dt-math>X_{t-1} \in \mathcal{I}</dt-math> to the allowed range <dt-math>\mathcal{I}_{s_t}</dt-math> is as follows: take out bits from <dt-math>X_{t-1}</dt-math> until it lands in the interval <dt-math>\mathcal{I}_{s_t}</dt-math>. Thus, effectively the mapping can be described as: 
  
  <dt-math block> 
  X'_{t-1} = \lfloor X_{t-1}/{2^r} \rfloor 
  </dt-math>
  where <dt-math> r </dt-math> is the smallest number, such that <dt-math>X'_{t-1} \in \mathcal{I}_{s_t}</dt-math> <br>
  Is this always possible? Are there cases where we will jump over the interval <dt-math>\mathcal{I}_{s_t}</dt-math>? For example: Let <dt-math> \mathcal{I}_A = \{ 5,6,7,8\}</dt-math>. If we start with a state of <dt-math> 17 </dt-math>, then on taking out bits we obtain the sequence: <dt-math> 17,8,4 </dt-math>, none of which lie in the <dt-math>\mathcal{I}_A</dt-math> range. This problem can be solved by choosing the intervals <dt-math>\mathcal{I}_{s_t}</dt-math> sufficiently large, so that we do not jump over them. So how large do we need to choose the intervals? One sufficient condition is the following: 
  <dt-math block>
  \forall a \in \mathcal{A}: \mathcal{I}_a = [L_a, H_a], H_a \geq 2L_a-1 
  </dt-math>
  Intuitively, every interval <dt-math> \mathcal{I}_a </dt-math> spans an octave: due to which it is not possible to jump over the intervals by taking out bits. Also note that we want this condition to be satisfied by all symbol ranges <dt-math> \mathcal{I}_a </dt-math>, corresponding to a <i> single </i> range <dt-math> I </dt-math>.  
  </p>
  <p>
  If we now think about how the decoding should proceed, the it is also clear from the mapping, that the "additional information" required by the decoder to reverse map <dt-math>X'_{t-1}</dt-math> to <dt-math>X_{t-1}</dt-math> is going to be the <dt-math> r </dt-math> bits, <dt-math> b_{t-1} = mod(X_{t-1},2^r) </dt-math> taken out of <dt-math>X_{t-1}</dt-math>, since: 
  <dt-math block> 
  X_{t-1} = X'_{t-1}*2^{r} + b_{t-1} 
  </dt-math>

  These bits <dt-math> b_{t-1}</dt-math> are be appended to a single <dt-math> BitStream </dt-math>, at every timestep. Note that, in general the decoder also needs to know the number of bits to pop out from this <dt-math> BitStream </dt-math>, since they can be different at different timesteps. One simple solution is to perform exactly the reverse of what the encoder does, i.e: Read in bits from the <dt-math> BitStream </dt-math>, and append them to the mapped state <dt-math> X'_{t-1} </dt-math> until it lands in the interval <dt-math>\mathcal{I}</dt-math>. Will this always work? One problem case is when there are more than <dt-math> 1 </dt-math> possible solutions for state <dt-math>X_{t-1}</dt-math> are in the valid interval <dt-math>\mathcal{I}</dt-math>, and the decoder stops early. For Example:
  <dt-math block>
  \begin{aligned}
   \mathcal{I} &= \{ 8,9,10,\ldots,16,17 \} \\
   X'_{t-1} &= 4 \\
   BitStream &= \{0100\ldots\} 
   \end{aligned}
   </dt-math>
   Then, the reverse mapped state <dt-math> X_{t-1} </dt-math> can be <dt-math> \{8,17,35,\ldots\} </dt-math>, out of which both <dt-math> 8,17 </dt-math> are valid states. Now, if the true state <dt-math> X_{t-1} = 17</dt-math>, then we are going to have an error as the decoder is going to stop after reading a single bit from the <dt-math>BitStream</dt-math> (as <dt-math>8 \in \mathcal{I}</dt-math>). This problem can be solved by choosing the interval <dt-math>\mathcal{I}</dt-math> small enough, so that there is only one possibility for the reverse mapping of <dt-math> X'_{t-1} </dt-math> to the state <dt-math> X_{t-1} </dt-math>. How small should <dt-math>\mathcal{I} </dt-math> be? One sufficient condition is: 
  <dt-math block>
  \mathcal{I} = [L, H], H \leq 2L-1 
  </dt-math>
  Notice that this condition is the exact opposite of what we got during the encoding for the symbol ranges! The intuition here is also analogous: adding bits increases the state more than twice, thus we choose the range <dt-math>\mathcal{I} </dt-math>, so that no number is larger than twice the other.
  </p>

  <h3> Streaming-rANS Final Form </h3>
  <p>
  Let us summarize all the conditions on the ranges we discussed in the previous section: 

  <dt-math block>
  \begin{aligned}
  &\forall a \in \mathcal{A}, \forall z \in \mathcal{I}_a : C_{rANS}(z,a) \in \mathcal{I} \\
  &\forall a \in \mathcal{A} : \mathcal{I}_a = [L_a, H_a], H_a \geq 2L_a-1 \\ 
  &\mathcal{I} = [L, H], H \leq 2L-1 
  \end{aligned}
  </dt-math>

  Are there some simple parameters for which we achieve all these conditions? You bet, there are! Jarek Duda, again in his paper <dt-cite key="duda2013asymmetric"></dt-cite>, gives a simple rule for which all these conditions are satisfied. For any integer <dt-math> l </dt-math>:
   <dt-math block>
  \begin{aligned}
  \mathcal{I} &= [lM, 2lM-1] \\
  \forall a \in \mathcal{A}: \mathcal{I}_{a} &= [lF_a, 2lF_a - 1]
  \end{aligned} 
   </dt-math>

  It is easy to see why this satisfies the second and third conditions mentioned above. The first condition is true since: <dt-math> C_{rANS}(lF_a,a) = lM   </dt-math> and <dt-math> C_{rANS}(2lF_a) = 2lM \notin \mathcal{I} </dt-math>. Phew! that was quite a bit of engineering to get rANS to get working in practice! For completeness, take a look at the python code for Streaming-rANS! You can also tryout the Streaming-rANS examples below. 
  </p>
  <h4> Streaming-rANS Encoding Example </h4>
   <p>
   Symbol Counts, <dt-math> \mathcal{F} </dt-math> <input type="text" id="symbol_counts_streaming" value="3,3,2"> <br>
   Input Symbol String: <input type="text" id="input_text_streaming" value="0,1,0,2,2,0,2,1,2"> <br>
   <button onclick="rANS_streaming_encoder()">Try it</button> 
   </p>
   <p id="rANS_stream_encoder"></p>

  <h4> Streaming-rANS Decoding Example </h4>
   <p>
   Symbol Counts, <dt-math> \mathcal{F} </dt-math> <input type="text" id="symbol_counts_streaming_decoder" value="3,3,2"> <br>
   Final State <input type="text" id="state_streaming_decoder" value="14"> <br>
   BitStream <input type="text" id="rANS_stream_decoder" value="0100000111000111"> <br>
   Number of Encoded Symbols <input type="text" id="num_streaming_decoder" value="9"> <br>
   <button onclick="rANS_streaming_decoder()">Try it</button> 
   </p>
   <p id="rANS_streaming_decoder_output"></p>


  <h4> Streaming-rANS as a FSE </h4>
   <p>
   Symbol Counts, <dt-math> \mathcal{F} </dt-math> <input type="text" id="symbol_counts_tANS" value="3,3,2"> <br>
   <button onclick="tANS_encoder()">Try it</button> 
   </p>
   <p id="tANS_encoder_output"></p>


</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf}
  }

  @article{duda2013asymmetric,
    title={Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding},
    author={Duda, Jarek},
    journal={arXiv preprint arXiv:1311.2540},
  year={2013}
  }

  @book{cover2012elements,
    title={Elements of information theory},
    author={Cover, Thomas M and Thomas, Joy A},
    year={2012},
    publisher={John Wiley \& Sons}
  }
</script>
